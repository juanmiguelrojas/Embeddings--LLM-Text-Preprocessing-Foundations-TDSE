{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v_e_eAxVk15h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings desde cero (Capítulo 2 - LLMs from Scratch)\n",
        "\n",
        "En este notebook implemento los conceptos clave del capítulo 2 del libro *Build a Large Language Model (From Scratch)* de Sebastian Raschka.\n",
        "\n",
        "El objetivo es entender cómo los LLM transforman texto en vectores numéricos (embeddings), ya que estos son la base de:\n",
        "\n",
        "- Modelos de lenguaje\n",
        "- Sistemas de recuperación semántica\n",
        "- Agentes inteligentes\n",
        "- RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "En lugar de usar librerías de alto nivel, reproducimos el proceso manualmente para entender:\n",
        "\n",
        "1. Tokenización\n",
        "2. Ventanas deslizantes\n",
        "3. Creación de datasets de entrenamiento\n",
        "4. Generación de embeddings con PyTorch"
      ],
      "metadata": {
        "id": "Iqs1JHfXrpRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch tiktoken --quiet"
      ],
      "metadata": {
        "id": "AzajQxHXr41V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "filename = \"the-verdict.txt\"\n",
        "\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Longitud del texto:\", len(text))\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsd2oiIzsDsv",
        "outputId": "8c5499ff-cf9f-4912-94b0-117383d9cb3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
            "\n",
            "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Por qué usar texto real?\n",
        "\n",
        "Entrenar modelos con texto real permite simular cómo funcionan los LLM en producción.\n",
        "\n",
        "El texto usado en el libro es un cuento corto que permite:\n",
        "\n",
        "- Tener un dataset pequeño y manejable\n",
        "- Ver patrones lingüísticos reales\n",
        "- Experimentar con tokenización realista\n",
        "\n",
        "Esto es importante porque los embeddings dependen directamente de la distribución del lenguaje. Un embedding entrenado con texto artificial no captura semántica real."
      ],
      "metadata": {
        "id": "G_oAI8SQsH76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "print(\"Número de tokens:\", len(tokens))\n",
        "print(tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ENCc7-hsOf1",
        "outputId": "e347a6a2-93ce-4e00-a1c1-0ca5dac79e08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 5145\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Por qué tokenizar es crítico?\n",
        "\n",
        "Los LLM no entienden texto directamente. Solo procesan números.\n",
        "\n",
        "La tokenización convierte texto en unidades discretas llamadas tokens. Estas pueden ser:\n",
        "\n",
        "- Palabras\n",
        "- Subpalabras\n",
        "- Caracteres\n",
        "\n",
        "En modelos modernos se usan subpalabras porque balancean:\n",
        "\n",
        "- Vocabulario manejable\n",
        "- Buena representación semántica\n",
        "\n",
        "Sin tokenización no existirían embeddings ni transformers, porque las redes neuronales solo operan sobre tensores numéricos."
      ],
      "metadata": {
        "id": "1rJfE46_sUAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "max_length = 4\n",
        "stride = 1\n",
        "\n",
        "input_ids = []\n",
        "target_ids = []\n",
        "\n",
        "for i in range(0, len(tokens) - max_length, stride):\n",
        "    input_chunk = tokens[i:i + max_length]\n",
        "    target_chunk = tokens[i + 1:i + max_length + 1]\n",
        "\n",
        "    input_ids.append(input_chunk)\n",
        "    target_ids.append(target_chunk)\n",
        "\n",
        "print(\"Número de muestras:\", len(input_ids))\n",
        "print(\"Ejemplo input:\", input_ids[0])\n",
        "print(\"Ejemplo target:\", target_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzJmR3DpsivJ",
        "outputId": "d827c9e3-a2c2-4843-b9ad-d42aa6c54449"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de muestras: 5141\n",
            "Ejemplo input: [40, 367, 2885, 1464]\n",
            "Ejemplo target: [367, 2885, 1464, 1807]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ventanas deslizantes en modelos de lenguaje\n",
        "\n",
        "Los LLM se entrenan prediciendo el siguiente token. Para ello se crean ventanas deslizantes.\n",
        "\n",
        "Ejemplo:\n",
        "Input:  [A, B, C, D]  \n",
        "Target: [B, C, D, E]\n",
        "\n",
        "Esto enseña al modelo dependencias secuenciales.\n",
        "\n",
        "El stride controla cuánto se mueve la ventana:\n",
        "- stride pequeño → más datos, más solapamiento\n",
        "- stride grande → menos datos, menos contexto compartido\n",
        "\n",
        "Este mecanismo es clave en:\n",
        "- Transformers\n",
        "- Modelos autoregresivos\n",
        "- Sistemas de generación de texto"
      ],
      "metadata": {
        "id": "xzuPBpuesvHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, input_ids, target_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.target_ids = target_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.input_ids[idx]), torch.tensor(self.target_ids[idx])\n",
        "\n",
        "dataset = GPTDataset(input_ids, target_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for x, y in dataloader:\n",
        "    print(\"Batch input shape:\", x.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DShENYZ-s1De",
        "outputId": "18718fa2-e3f4-436c-9758-18a377139916"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch input shape: torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importancia de estructurar datasets\n",
        "\n",
        "Convertir datos en un Dataset de PyTorch permite:\n",
        "\n",
        "- Entrenamiento eficiente en GPU\n",
        "- Batching automático\n",
        "- Paralelismo\n",
        "\n",
        "Esto es exactamente lo que hacen frameworks como:\n",
        "- HuggingFace\n",
        "- DeepSpeed\n",
        "- SageMaker Training Jobs\n",
        "\n",
        "Entender esta capa es clave para construir agentes propios o entrenar modelos personalizados."
      ],
      "metadata": {
        "id": "EMVD6aums8Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "embedding_dim = 256\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "sample_input = torch.tensor(input_ids[:2])\n",
        "embedded = embedding_layer(sample_input)\n",
        "\n",
        "print(\"Shape embeddings:\", embedded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4JVmY1yuCev",
        "outputId": "b160792b-6f43-45c4-c46b-e55a51a7fe0e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape embeddings: torch.Size([2, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué son embeddings?\n",
        "\n",
        "Un embedding es una representación vectorial densa de un token.\n",
        "\n",
        "En lugar de usar one-hot encoding (vectores enormes y dispersos), los embeddings:\n",
        "\n",
        "- Reducen dimensionalidad\n",
        "- Capturan relaciones semánticas\n",
        "- Permiten generalización\n",
        "\n",
        "Ejemplo conceptual:\n",
        "rey - hombre + mujer ≈ reina\n",
        "\n",
        "Esto emerge porque los embeddings aprenden patrones estadísticos del lenguaje."
      ],
      "metadata": {
        "id": "Nl3jgSr5uH1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Por qué los embeddings codifican significado y cómo se relacionan con redes neuronales?\n",
        "\n",
        "Los embeddings codifican significado porque se entrenan mediante optimización de redes neuronales profundas.\n",
        "\n",
        "Durante el entrenamiento:\n",
        "1. El modelo predice el siguiente token\n",
        "2. Calcula error (loss)\n",
        "3. Backpropagation ajusta pesos\n",
        "\n",
        "La capa de embedding es simplemente una matriz entrenable. Cada fila corresponde a un token.\n",
        "\n",
        "Con el tiempo:\n",
        "- Tokens usados en contextos similares convergen en regiones cercanas del espacio vectorial\n",
        "- Esto crea geometría semántica\n",
        "\n",
        "Relación con conceptos de NN:\n",
        "- Es una capa lineal entrenable\n",
        "- Aprende representaciones latentes\n",
        "- Funciona como reducción de dimensionalidad supervisada\n",
        "\n",
        "Por eso los embeddings son la base de:\n",
        "- Búsqueda semántica\n",
        "- RAG\n",
        "- Agentes LLM\n",
        "- Sistemas multimodales"
      ],
      "metadata": {
        "id": "UhpWN-TZuSJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_samples(max_length, stride):\n",
        "    count = 0\n",
        "    for i in range(0, len(tokens) - max_length, stride):\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "configs = [\n",
        "    (4, 1),\n",
        "    (4, 2),\n",
        "    (8, 1),\n",
        "    (8, 4),\n",
        "]\n",
        "\n",
        "for max_len, stride_val in configs:\n",
        "    samples = count_samples(max_len, stride_val)\n",
        "    print(f\"max_length={max_len}, stride={stride_val} → muestras={samples}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx2UlyTsub2O",
        "outputId": "9f509333-cca4-4f41-fe2b-f003f0884451"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_length=4, stride=1 → muestras=5141\n",
            "max_length=4, stride=2 → muestras=2571\n",
            "max_length=8, stride=1 → muestras=5137\n",
            "max_length=8, stride=4 → muestras=1285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimento: efecto de max_length y stride\n",
        "\n",
        "Resultados observados:\n",
        "- Reducir stride aumenta el número de muestras\n",
        "- Aumentar max_length reduce el número de ventanas posibles\n",
        "\n",
        "¿Por qué ocurre esto?\n",
        "\n",
        "Número de muestras ≈ (N - max_length) / stride\n",
        "\n",
        "### Interpretación\n",
        "\n",
        "Stride pequeño:\n",
        "- Más datos de entrenamiento\n",
        "- Más solapamiento\n",
        "- Mejor aprendizaje contextual\n",
        "\n",
        "Stride grande:\n",
        "- Menos redundancia\n",
        "- Entrenamiento más rápido\n",
        "- Menor captura de dependencias largas\n",
        "\n",
        "### ¿Por qué el solapamiento es útil?\n",
        "\n",
        "El solapamiento permite que el modelo vea el mismo token en múltiples contextos.\n",
        "\n",
        "Esto mejora:\n",
        "- Robustez semántica\n",
        "- Generalización\n",
        "- Calidad de embeddings\n",
        "\n",
        "Este principio también se usa en:\n",
        "- Sliding window attention\n",
        "- Chunking en RAG\n",
        "- Procesamiento de documentos largos\n",
        "\n",
        "## Conclusiones\n",
        "\n",
        "En este laboratorio implementé desde cero el pipeline base de embeddings:\n",
        "\n",
        "- Tokenización\n",
        "- Ventanas deslizantes\n",
        "- Dataset en PyTorch\n",
        "- Capa de embeddings\n",
        "\n",
        "Esto demuestra que los embeddings no son magia, sino matrices entrenables que emergen de optimización neuronal.\n",
        "\n",
        "Entender este proceso es fundamental para:\n",
        "- Construir LLMs propios\n",
        "- Diseñar sistemas RAG\n",
        "- Desarrollar agentes autónomos\n",
        "- Optimizar despliegues en SageMaker o producción\n",
        "\n",
        "Este conocimiento conecta teoría y práctica, permitiendo pasar de usuario de LLMs a constructor de modelos."
      ],
      "metadata": {
        "id": "lxZKl5pOuiPn"
      }
    }
  ]
}